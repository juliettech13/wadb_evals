{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMs in production - Trace, Compile, Evals - by Weights & Biases\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tcapelle/llm-evals-workshop/blob/main/eval.ipynb) [![Weights & Biases](https://raw.githubusercontent.com/wandb/assets/main/wandb-github-badge-gradient.svg)](https://wandb.ai/capecape/evals-workshop/weave/traces)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Intro\n",
    "This notebook is accompanying a workshop, that will walk you through common patterns in building evaluations for LLMs, and useful rules of thumb to follow when doing so using [W&B Weave](https://wandb.me/weave-workshop-jan)\n",
    "\n",
    "We'll explore the following methodology for productizing robust LLM applications: \n",
    "\n",
    "![three](https://gist.github.com/user-attachments/assets/0d51de65-8ec7-4cc5-a102-5a13229f5531)\n",
    "\n",
    "\n",
    "Make sure to set your WANDB_API_KEY (get your key from [here](https://wandb.ai/authorize)) and OPENAI_API_KEY (if you have that) in the environment variables.\n",
    "\n",
    "If you're running in Colab, set the variables in the keys section on the left. \n",
    "\n",
    "Prepared by [Alex Volkov](https://twitter.com/altryne) and [Thomas](https://tcapelle.github.io/socials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and read in required packages\n",
    "try:\n",
    "    import google.colab\n",
    "    !git clone -q --branch main https://github.com/tcapelle/llm-evals-workshop\n",
    "    %cd llm-evals-workshop\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "print('‚è≥ Installing packages')\n",
    "%pip install -q uv\n",
    "!uv pip install -q --system weave gradio set-env-colab-kaggle-dotenv tqdm ipywidgets requests openai pillow litellm\n",
    "print('‚úÖ Packages installed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup weave and the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in as Weights & Biases user: capecape.\n",
      "View Weave data at https://wandb.ai/capecape/evals-workshop/weave\n"
     ]
    }
   ],
   "source": [
    "%load_ext gradio\n",
    "\n",
    "from set_env import set_env\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import weave\n",
    "\n",
    "load_dotenv()\n",
    "set_env(\"WANDB_API_KEY\")\n",
    "set_env(\"OPENAI_API_KEY\")\n",
    "\n",
    "# initialize weave\n",
    "weave_api = weave.init('evals-workshop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our LLM client\n",
    "from litellm import completion\n",
    "\n",
    "model = \"openai/gpt-4o\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tracing LLM calls with Weave\n",
    "\n",
    "#### Why Tracing is Important for LLM Application Reliability\n",
    "\n",
    "In building reliable LLM-based applications, having a clear view into\n",
    "how your system behaves is crucial. That‚Äôs where ‚Äútracing‚Äù comes in.\n",
    "\n",
    "1. **Detailed Interaction Records**:\n",
    "   Tracing captures all the inputs, prompts, responses, and any user feedback.\n",
    "   By preserving this detailed record, you always have the context needed to\n",
    "   debug unexpected or incorrect results.\n",
    "\n",
    "2. **Rapid Issue Diagnosis**:\n",
    "   With thorough traces, you can pinpoint issues faster‚Äîoften without\n",
    "   needing direct access to remote systems. Simply reviewing the logs can\n",
    "   reveal how a certain response was triggered.\n",
    "\n",
    "3. **Collaboration and Sharing**:\n",
    "   Traces can be shared with both technical and non-technical stakeholders.\n",
    "   This not only streamlines collaboration but also ensures everyone is\n",
    "   working off the same ‚Äúsource of truth‚Äù when investigating bugs\n",
    "   or brainstorming improvements.\n",
    "\n",
    "4. **Outlier Spotting and Performance Tuning**:\n",
    "   By tracking calls at scale, you can detect when responses deviate\n",
    "   dramatically from the norm, troubleshoot any failures, and identify\n",
    "   potential performance bottlenecks.\n",
    "\n",
    "5. **Facilitates Product Evolution**:\n",
    "   As you enhance or expand your LLM application, comprehensive\n",
    "   tracing data helps you make more informed decisions about what to\n",
    "   improve, remove, or refine.\n",
    "\n",
    "With W&B Weave, comprehensive tracing is just 1 line of code, and offers features such as:\n",
    "- Syntax highlighting specific to your use-case (Markdown, JSON, etc.)\n",
    "- Ability to share links with other members of your team\n",
    "- Ability to filter traces by function name, input, output, etc.\n",
    "- Tracking latency, token count and cost per call (and trends)\n",
    "- Code associated with the llm call and versioning\n",
    "- Ability to add metadata per trace\n",
    "\n",
    "If you need to instrument existing code, you can use the `@weave.op` decorator to trace the function.  \n",
    "\n",
    "![CleanShot 2024-04-08 at 14 15 40@2x](https://gist.github.com/assets/463317/4e9ada49-572f-47d9-91e1-55ab72b2a476)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op  # üí™\n",
    "def analyze_post_sentiment(avatar, displayName, text):\n",
    "    # Prompt for OpenAI to analyze the sentiment\n",
    "\n",
    "    prompt = f\"\"\"Analyze the following Bluesky post and determine if the author is a:\n",
    "    - DOOMER (someone who hates AI and uses derogatory language)\n",
    "    - BOOMER (someone who doesn't understand AI and asks to remove their data)\n",
    "    - NEITHER (neutral or positive response)\n",
    "    \n",
    "    Post: {displayName}: \"{text}\"\n",
    "    \n",
    "    Respond with just one word (DOOMER, BOOMER, or NEITHER) followed by a brief explanation.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = completion(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.5\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        current_call = weave.require_current_call()\n",
    "        weave_call_id = current_call.id\n",
    "    except:\n",
    "        weave_call_id = None\n",
    "    \n",
    "    return {\n",
    "        \"llm_classification\": response.choices[0].message.content,\n",
    "        \"weave_call_id\": weave_call_id\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üç© https://wandb.ai/capecape/evals-workshop/r/call/019490e8-9d9c-7663-9bc3-816e81e11bc9\n",
      "{'llm_classification': \"DOOMER: The post explicitly states hatred towards AI, which aligns with a DOOMER's negative attitude and derogatory stance towards AI.\", 'weave_call_id': '019490e8-9d9c-7663-9bc3-816e81e11bc9'}\n"
     ]
    }
   ],
   "source": [
    "# Lets test this out without tracing first\n",
    "response_dict = analyze_post_sentiment(\"\",\"Alex\",\"I hate AI\")\n",
    "\n",
    "print(response_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that even without `@weave.op`, since Weave is initialized, it will still trace the function call and store it in the Weave project as it automatically understands that we use the OpenAI client. However, if we add `@weave.op`, we can get even more detail and instrument our existing code with Weave.\n",
    " \n",
    "Tracing becomes even more useful when you have a lot of nested calls, such as a multi-step chat conversation, a RAG system with retrieval, or an agentic system with multiple steps.\n",
    "\n",
    "![text](https://cln.sh/Sc8ZtrdM+)\n",
    "\n",
    "[Here's a great example](https://wandb.ai/wandb-designers/winston/weave/traces?cols=%7B%22attributes.weave.client_version%22%3Afalse%2C%22attributes.weave.os_name%22%3Afalse%2C%22attributes.weave.os_release%22%3Afalse%2C%22attributes.weave.os_version%22%3Afalse%2C%22attributes.weave.source%22%3Afalse%2C%22attributes.weave.sys_version%22%3Afalse%7D&peekPath=%2Fwandb-designers%2Fwinston%2Fcalls%2F0193ff3f-54d7-73a3-8004-0a582a594307%3Fpath%3Dwinston-solve*0%2Bvincent-execute*0%26tracetree%3D1) of a more complex traced setup from our internal agent system called Winston - with multiple tools selection, retrieval steps etc Winston Weave Dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2. User Feedback & Annotations\n",
    "\n",
    "Collecting user feedback is a crucial way to improve your LLM applications. There's a reason that every chatbot you use has üëç/üëé and a text box to leave feedback. This is one of the best ways for those labs to understand and improve their models and align them to user preferences.\n",
    "\n",
    "![text](https://cln.sh/JGMBxMtH+)\n",
    "\n",
    "Users don't have to be external as well, as you develop your application, marking traces as \"good\" or \"bad\", and adding why, is a great way to kick start your initial evaluation dataset with working and non-working examples. \n",
    "\n",
    "Additionally, after logging hundreds of thousads of traces, they will all start looking the same, so additional context like your user's feedback, will greately improve your ability to look at your data and find the outliers.\n",
    "\n",
    "Weave supports collecting user Feedback in the UI and also via the API so you can collect it from your users and also leave it yourself while looking at your data. \n",
    "\n",
    "![text](https://cln.sh/X6fFHD8t+)\n",
    "\n",
    "Read more about feedback [here](https://weave-docs.wandb.ai/guides/tracking/feedback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Published to https://wandb.ai/capecape/evals-workshop/weave/objects/doomer_or_boomer/versions/MfppDkza1qvK772eNZWIU1XwwZbtwGQ8UQWWEcyZlfc\n",
      "üì¶ Published to https://wandb.ai/capecape/evals-workshop/weave/objects/reason/versions/Z3Do6YnUa9YHEuELfGyZtJt7JTkgb30oVBv04U4HWyc\n"
     ]
    }
   ],
   "source": [
    "# @title { display-mode: \"form\" }\n",
    "import os\n",
    "import gradio as gr\n",
    "from PIL import Image\n",
    "import requests \n",
    "import io\n",
    "import json\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "from datetime import datetime\n",
    "import random\n",
    "import weave\n",
    "from weave.flow.annotation_spec import AnnotationSpec\n",
    "\n",
    "# initialize annotations for this project\n",
    "annotation = weave.publish(AnnotationSpec(\n",
    "    name=\"Doomer or Boomer\",\n",
    "    description=\"Doomer or Boomer or Neither\",\n",
    "    field_schema={ \"type\": \"string\", \"enum\": [\"Doomer\", \"Boomer\", \"Neither\"],},\n",
    "), \"doomer_or_boomer\")\n",
    "\n",
    "annotation_reason = weave.publish(AnnotationSpec(\n",
    "    name=\"Reason\",\n",
    "    description=\"Reason why you chose this value, write before clicking.\",\n",
    "    field_schema={ \"type\": \"string\"},\n",
    "), \"reason\")\n",
    "\n",
    "\n",
    "# cell 2\n",
    "# Load the Jinja2 environment\n",
    "env = Environment(loader=FileSystemLoader('templates'))\n",
    "template = env.get_template('post.html.jinja')\n",
    "\n",
    "# Load replies data\n",
    "def load_replies():\n",
    "    replies = []\n",
    "    # Load replies from both files\n",
    "    with open('data/replies_alpin.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "        replies.extend(data['thread']['replies'])\n",
    "    with open('data/replies_daniel.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "        replies.extend(data['thread']['replies'])\n",
    "    return replies\n",
    "\n",
    "\n",
    "def get_random_post_and_analyze():\n",
    "    replies = load_replies()\n",
    "    post = random.choice(replies)\n",
    "    \n",
    "    # Format the post data for the template\n",
    "    created_at = datetime.fromisoformat(post['post']['record']['createdAt'].replace('Z', '+00:00'))\n",
    "    formatted_date = created_at.strftime('%b %d, %Y, %I:%M %p')\n",
    "    \n",
    "    # Convert AT URI to bsky.app URL\n",
    "    at_uri = post['post']['uri']\n",
    "    _, _, author_did, _, post_id = at_uri.split('/')\n",
    "    post_url = f\"https://bsky.app/profile/{post['post']['author']['handle']}/post/{post_id}\"\n",
    "    \n",
    "    # Analyze the post\n",
    "    #download the avatar and convert to PIL image\n",
    "    avatar_uri = post['post']['author'].get('avatar')\n",
    "    avatar_response = requests.get(avatar_uri)\n",
    "    avatar_pil = Image.open(io.BytesIO(avatar_response.content))\n",
    "\n",
    "    response_dict = analyze_post_sentiment(avatar_pil, post['post']['author']['displayName'], post['post']['record']['text'])\n",
    "    analysis = response_dict['llm_classification']\n",
    "    weave_call_id = response_dict['weave_call_id']\n",
    "    \n",
    "    post_data = {\n",
    "        'author': post['post']['author'],\n",
    "        'created_at': formatted_date,\n",
    "        'text': post['post']['record']['text'],\n",
    "        'like_count': post['post'].get('likeCount', 0),\n",
    "        'repost_count': post['post'].get('repostCount', 0),\n",
    "        'has_image': False,\n",
    "        'post_url': post_url\n",
    "    }\n",
    "    \n",
    "    return template.render(**post_data), analysis, weave_call_id, ''\n",
    "\n",
    "\n",
    "def submit_feedback(user_selection, reason, weave_call_id):\n",
    "    \"\"\"\n",
    "    Example function that could send user feedback (the user_selection)\n",
    "    and the weave_call_id to your Weave (or any other) API.\n",
    "    \"\"\"\n",
    "    call = weave_api.get_call(weave_call_id)\n",
    "    \n",
    "    if not call:\n",
    "        raise Exception('No Weave call ID found, have you tried adding @weave.op to the analyze_post_sentiment function?')\n",
    "    \n",
    "    if reason:\n",
    "        reason_resp = weave_api.server.feedback_create(\n",
    "            {\n",
    "            \"project_id\": weave_api._project_id(),\n",
    "            \"weave_ref\": call.ref.uri(),\n",
    "            \"feedback_type\": \"wandb.annotation.reason\",\n",
    "            \"annotation_ref\": annotation_reason.uri(),\n",
    "            \"payload\": {\"value\": reason},\n",
    "            }\n",
    "        )\n",
    "\n",
    "    resp = weave_api.server.feedback_create(\n",
    "        {\n",
    "            \"project_id\": weave_api._project_id(),\n",
    "            \"weave_ref\": call.ref.uri(),\n",
    "            \"feedback_type\": \"wandb.annotation.doomer_or_boomer\",\n",
    "            \"annotation_ref\": annotation.uri(),\n",
    "            \"payload\": {\"value\": user_selection},\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Ready to analyze the next post\n",
    "    return get_random_post_and_analyze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 2.1 Doomer or Boomer App - Annotations by example\n",
    "\n",
    "Unlike user feedback, Annotations are a bit of a more structure way to classify responses, to help create a dataset of golden answers and reasons or rationales for those answers. All of the major companies use Scale.ai for this and pay them a LOT of money, but you don't have to right away, you can start small, by yourself or with your team. \n",
    "\n",
    "Let's see how we can kickstart a simple dataset of annotations by a practical example.\n",
    "\n",
    "![image](https://gist.github.com/user-attachments/assets/a8537545-e070-4c8e-9988-2a8a905b9d2c)\n",
    "\n",
    "To simulate a real world scenario, we'll build a simple app that will allow you to annotate a few posts. \n",
    "\n",
    "In our case, we're pretending to work at a company that's trying to build an AI classifier for Bluesky posts. We're humans that work in the company and are helping it to align and finetune models for AI moderation. \n",
    "\n",
    "We've compiled replies from BlueSky users, on 2 posts that collected publicly available data from BlieSky to train AI models (BlueSky data is public), which led to a lot of hate by users on BlueSky. \n",
    "\n",
    "We're going to build a simple app that will use an LLM to classify the replies into 3 categories: `Doomer`, `Boomer`, or `Neither`. \n",
    "\n",
    "`Doomer`: Someone who hates AI, and uses derogatory language towards the author of the post because of thier hate for AI and their data being used for AI  \n",
    "`Boomer`: Someone who doesn't understand AI, and copy-pastes a request to remove their data from the dataset  \n",
    "`Neither`: Folks who reply neutral or positive to the post.\n",
    "\n",
    "At first our LLMs will not have context to the task, so won't be able to reliably classify the replies, so a human is needed to annotate with additional context, you are that human. \n",
    "\n",
    "Launch the app and go through a few posts, annotate with a reason for your choice and the correct classification, we'll later use this data to align/finetune our LLM to classify the replies more accuretly and reliably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%blocks\n",
    "# TODO - Launch the Gradio app and annotate 10-20 examples according to the rules\n",
    "os.environ['WEAVE_PRINT_CALL_LINK'] = 'false'\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    # Add a title and description\n",
    "    gr.Markdown(\"\"\"\n",
    "    # ü¶ã Doomer or Boomer\n",
    "    Our AI analyzes bluesky replies and posts to determine if the author is a doomer or a boomer.  \n",
    "    Source of data: Replies to a post by a BlueSky user that compiled a dataset of posts, which went viral and generated a lot of hate on BlueSky.  \n",
    "    These are replies and comments on 2 posts that collected a dataset of posts of BlueSky users to train AI models (BlueSky data is public)\n",
    "    \"\"\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=2):\n",
    "            post_html = gr.HTML()\n",
    "            next_post_btn = gr.Button(\"Skip Post & Analyze Another\", variant=\"primary\")\n",
    "            gr.Markdown(f\"\"\"\n",
    "            #### Instructions for labeler: \n",
    "            `Doomer`: Someone who hates AI, and uses derogatory language towards the author of the post because of thier hate for AI and their data being used for AI  \n",
    "            `Boomer`: Someone who doesn't understand AI, and copy-pastes a request to remove their data from the dataset  \n",
    "            `Neither`: Folks who reply neutral or positive to the post.\n",
    "            \n",
    "            See your Weave project & traces [here](https://wandb.ai/{weave_api._project_id()})\n",
    "            \"\"\")\n",
    "        \n",
    "        with gr.Column(scale=1):\n",
    "            analysis_output = gr.Textbox(\n",
    "                label=\"Analysis Results\",\n",
    "                placeholder=\"Analysis will appear here...\",\n",
    "                lines=4\n",
    "            )\n",
    "            weave_call_id_state = gr.State()\n",
    "            \n",
    "            # Replace dropdown with three buttons\n",
    "            reason_input = gr.Textbox(label=\"Add reason and click\",placeholder=\"Reason why you chose this value, write before clicking.\", lines=2)\n",
    "            with gr.Row():\n",
    "                doomer_btn = gr.Button(\"Doomer üò°\", variant=\"huggingface\")\n",
    "                boomer_btn = gr.Button(\"Boomer üëµ\", variant=\"primary\")\n",
    "                neither_btn = gr.Button(\"Neither ü§∑\")\n",
    "\n",
    "            \n",
    "    # Set up event handler for combined next/analyze\n",
    "    next_post_btn.click(fn=get_random_post_and_analyze, outputs=[post_html, analysis_output, weave_call_id_state, reason_input])\n",
    "    \n",
    "    doomer_btn.click(\n",
    "    fn=submit_feedback,\n",
    "    inputs=[gr.State(\"Doomer\"), reason_input, weave_call_id_state],\n",
    "    outputs=[post_html, analysis_output, weave_call_id_state, reason_input]\n",
    "    )\n",
    "    boomer_btn.click(\n",
    "        fn=submit_feedback,\n",
    "        inputs=[gr.State(\"Boomer\"), reason_input, weave_call_id_state],\n",
    "        outputs=[post_html, analysis_output, weave_call_id_state, reason_input]\n",
    "    )\n",
    "    neither_btn.click(\n",
    "        fn=submit_feedback,\n",
    "        inputs=[gr.State(\"Neither\"), reason_input, weave_call_id_state],\n",
    "        outputs=[post_html, analysis_output, weave_call_id_state, reason_input]\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Initialize with first post and analysis\n",
    "    post_html.value, analysis_output.value, weave_call_id_state.value, reason_input.value = get_random_post_and_analyze()\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Building a dataset from annotated calls\n",
    "\n",
    "Now that we've annotated at least 10-20 examples, we can build our first evaluation dataset! \n",
    "\n",
    "![text](https://cln.sh/dyBq4QXD+)\n",
    "\n",
    "Step 1: Filter calls in Weave UI by only those with annotations not empty\n",
    "\n",
    "Step 2: Use the Export -> Use Python button to get code to extract a list of filtered annotated calls\n",
    "\n",
    "Step 3: Convert the calls to a clean evaluation dataset (and optionally publish to Weave)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op\n",
    "def get_annotated_calls():\n",
    "   # Weave API call to get all calls filtered by annotations not empty (with reasons)\n",
    "   resp = weave_api.server.calls_query_stream({\n",
    "      \"project_id\": weave_api._project_id(),\n",
    "      \"filter\": {\"op_names\": [f\"weave:///{weave_api._project_id()}/op/analyze_post_sentiment:*\"]},\n",
    "      \"query\": {\"$expr\":{\"$and\":[{\"$not\":[{\"$eq\":[{\"$getField\":\"feedback.[wandb.annotation.doomer_or_boomer].payload.value\"},{\"$literal\":\"\"}]}]},{\"$not\":[{\"$eq\":[{\"$getField\":\"feedback.[wandb.annotation.reason].payload.value\"},{\"$literal\":\"\"}]}]}]}},\n",
    "      \"sort_by\": [{\"field\":\"started_at\",\"direction\":\"desc\"}],\n",
    "      \"include_feedback\": True,\n",
    "   })\n",
    "\n",
    "   # Iterate over the calls, clean up and publish as a dataset we can version and reference later.\n",
    "   list_of_calls = []\n",
    "   dataset = []\n",
    "   for call in resp:\n",
    "      try:\n",
    "         row = {}\n",
    "         call_dict = dict(call)\n",
    "         row[\"input\"] = call_dict.get('inputs').get('text')\n",
    "         row[\"displayName\"] = call_dict.get('inputs').get('displayName')\n",
    "         row[\"llm_classification\"] = call_dict.get('output').get('llm_classification')\n",
    "         list_of_feedback = call_dict.get('summary').get('weave').get('feedback')\n",
    "         for feedback in list_of_feedback:\n",
    "            if feedback.get(\"feedback_type\") == 'wandb.annotation.doomer_or_boomer':\n",
    "               row[\"human_annotation\"] = feedback.get('payload').get('value')\n",
    "            if feedback.get(\"feedback_type\") == 'wandb.annotation.reason':\n",
    "               row[\"reason\"] = feedback.get('payload').get('value')\n",
    "      except Exception as e:\n",
    "        continue\n",
    "      \n",
    "      dataset.append(row)\n",
    "\n",
    "   return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üç© https://wandb.ai/capecape/evals-workshop/r/call/019490e9-3fe4-7b10-b67d-a11d884e397f\n",
      "16\n",
      "üì¶ Published to https://wandb.ai/capecape/evals-workshop/weave/objects/doomer_or_boomer_dataset/versions/AjXt0JMXffKov9O8MUU2oyr2oVTLYTjIQ9XEva0ZEus\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectRef(entity='capecape', project='evals-workshop', name='doomer_or_boomer_dataset', _digest='AjXt0JMXffKov9O8MUU2oyr2oVTLYTjIQ9XEva0ZEus', _extra=())"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = get_annotated_calls()\n",
    "print(len(dataset))\n",
    "\n",
    "weave.publish(weave.Dataset(name=\"doomer_or_boomer_dataset\", rows=dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Storing Datasets within Weave\n",
    "\n",
    "If you'd like to store your own dataset and name them, it's very easy to do so, and then you get a \"ref\" to the dataset that's stored in our system. Weave datasets are versioned, which means you can reference them in your code by a URL or a ref, and either point to the latest version or a specific version. \n",
    "\n",
    "Using `refs` is a great way to make your code reproducible and versioned.\n",
    "\n",
    "![CleanShot 2025-01-07 at 16 12 35@2x](https://gist.github.com/user-attachments/assets/e2d02340-cc0f-41e8-8d97-957b08611d08)\n",
    "\n",
    "\n",
    "Here's an example of the dataset we just created, and how we can reuse it in our evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>displayName</th>\n",
       "      <th>llm_classification</th>\n",
       "      <th>reason</th>\n",
       "      <th>human_annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I request that any of my data that is containe...</td>\n",
       "      <td>Kathryn Tewson</td>\n",
       "      <td>BOOMER - The author is requesting the removal ...</td>\n",
       "      <td>fear of data</td>\n",
       "      <td>Boomer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Researchers\"</td>\n",
       "      <td>Ryan</td>\n",
       "      <td>DOOMER: The use of quotation marks around \"Res...</td>\n",
       "      <td>hate against the researcher</td>\n",
       "      <td>Doomer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Do not waiver</td>\n",
       "      <td>phi</td>\n",
       "      <td>NEITHER\\n\\nThe post \"phi: 'Do not waiver'\" doe...</td>\n",
       "      <td>nothing</td>\n",
       "      <td>Neither</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lmao</td>\n",
       "      <td>wireless_anon</td>\n",
       "      <td>NEITHER\\n\\nThe post \"lmao\" is a neutral expres...</td>\n",
       "      <td>nothing</td>\n",
       "      <td>Neither</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I hope you become resistant to Adderall</td>\n",
       "      <td>Bro Laren</td>\n",
       "      <td>NEITHER\\n\\nThe post does not contain any expli...</td>\n",
       "      <td>nothing</td>\n",
       "      <td>Neither</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>This is garbage, you're a thief, and you disgu...</td>\n",
       "      <td>Hank Single</td>\n",
       "      <td>DOOMER\\n\\nThe post contains derogatory languag...</td>\n",
       "      <td>Hate to AI</td>\n",
       "      <td>Doomer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>If you‚Äôve got any of my data then you should r...</td>\n",
       "      <td>Sanitary Naptime</td>\n",
       "      <td>BOOMER: The post expresses a desire to have pe...</td>\n",
       "      <td>fear of data and hate</td>\n",
       "      <td>Boomer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I request that any and all of my data (or ment...</td>\n",
       "      <td>Chip</td>\n",
       "      <td>BOOMER\\n\\nExplanation: The author is requestin...</td>\n",
       "      <td>fear of data</td>\n",
       "      <td>Boomer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hi I do not consent for my posts or content to...</td>\n",
       "      <td>Ness (they/them)</td>\n",
       "      <td>NEITHER\\n\\nThe post expresses a clear request ...</td>\n",
       "      <td>fear of data</td>\n",
       "      <td>Boomer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Didn‚Äôt we all leave twitter in part because of...</td>\n",
       "      <td>Your Favorite Southern Belle Stoner Mom</td>\n",
       "      <td>NEITHER\\n\\nThe post expresses frustration with...</td>\n",
       "      <td>frustration</td>\n",
       "      <td>Neither</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>This sucks</td>\n",
       "      <td>David Brunelle</td>\n",
       "      <td>NEITHER\\n\\nExplanation: The post \"This sucks\" ...</td>\n",
       "      <td>nothing</td>\n",
       "      <td>Boomer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I do not consent to have my data used by this ...</td>\n",
       "      <td>Kyle Poddig (He/They/Them)</td>\n",
       "      <td>BOOMER: The post expresses a lack of understan...</td>\n",
       "      <td>fear of data</td>\n",
       "      <td>Boomer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Hello, please inform me where to find your pro...</td>\n",
       "      <td>extra haruspicy</td>\n",
       "      <td>NEITHER: The post is a straightforward request...</td>\n",
       "      <td>Fear of data collection</td>\n",
       "      <td>Boomer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Rethink your life. Grow and be better.</td>\n",
       "      <td>Peter</td>\n",
       "      <td>NEITHER: The post encourages personal growth a...</td>\n",
       "      <td>Hater</td>\n",
       "      <td>Boomer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>I request that any of my data that is containe...</td>\n",
       "      <td>The Pin is mightier than the sword</td>\n",
       "      <td>NEITHER: The post is a request for data deleti...</td>\n",
       "      <td>Fear of data collection</td>\n",
       "      <td>Boomer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Open source doesn‚Äôt mean you can do shit witho...</td>\n",
       "      <td>Six does sex work adjacent aspirational lifest...</td>\n",
       "      <td>NEITHER: The post discusses a perspective on o...</td>\n",
       "      <td>Hater on OSS</td>\n",
       "      <td>Boomer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                input  \\\n",
       "0   I request that any of my data that is containe...   \n",
       "1                                       \"Researchers\"   \n",
       "2                                       Do not waiver   \n",
       "3                                                lmao   \n",
       "4             I hope you become resistant to Adderall   \n",
       "5   This is garbage, you're a thief, and you disgu...   \n",
       "6   If you‚Äôve got any of my data then you should r...   \n",
       "7   I request that any and all of my data (or ment...   \n",
       "8   Hi I do not consent for my posts or content to...   \n",
       "9   Didn‚Äôt we all leave twitter in part because of...   \n",
       "10                                         This sucks   \n",
       "11  I do not consent to have my data used by this ...   \n",
       "12  Hello, please inform me where to find your pro...   \n",
       "13             Rethink your life. Grow and be better.   \n",
       "14  I request that any of my data that is containe...   \n",
       "15  Open source doesn‚Äôt mean you can do shit witho...   \n",
       "\n",
       "                                          displayName  \\\n",
       "0                                      Kathryn Tewson   \n",
       "1                                                Ryan   \n",
       "2                                                 phi   \n",
       "3                                       wireless_anon   \n",
       "4                                           Bro Laren   \n",
       "5                                         Hank Single   \n",
       "6                                    Sanitary Naptime   \n",
       "7                                                Chip   \n",
       "8                                    Ness (they/them)   \n",
       "9             Your Favorite Southern Belle Stoner Mom   \n",
       "10                                     David Brunelle   \n",
       "11                         Kyle Poddig (He/They/Them)   \n",
       "12                                    extra haruspicy   \n",
       "13                                              Peter   \n",
       "14                 The Pin is mightier than the sword   \n",
       "15  Six does sex work adjacent aspirational lifest...   \n",
       "\n",
       "                                   llm_classification  \\\n",
       "0   BOOMER - The author is requesting the removal ...   \n",
       "1   DOOMER: The use of quotation marks around \"Res...   \n",
       "2   NEITHER\\n\\nThe post \"phi: 'Do not waiver'\" doe...   \n",
       "3   NEITHER\\n\\nThe post \"lmao\" is a neutral expres...   \n",
       "4   NEITHER\\n\\nThe post does not contain any expli...   \n",
       "5   DOOMER\\n\\nThe post contains derogatory languag...   \n",
       "6   BOOMER: The post expresses a desire to have pe...   \n",
       "7   BOOMER\\n\\nExplanation: The author is requestin...   \n",
       "8   NEITHER\\n\\nThe post expresses a clear request ...   \n",
       "9   NEITHER\\n\\nThe post expresses frustration with...   \n",
       "10  NEITHER\\n\\nExplanation: The post \"This sucks\" ...   \n",
       "11  BOOMER: The post expresses a lack of understan...   \n",
       "12  NEITHER: The post is a straightforward request...   \n",
       "13  NEITHER: The post encourages personal growth a...   \n",
       "14  NEITHER: The post is a request for data deleti...   \n",
       "15  NEITHER: The post discusses a perspective on o...   \n",
       "\n",
       "                         reason human_annotation  \n",
       "0                  fear of data           Boomer  \n",
       "1   hate against the researcher           Doomer  \n",
       "2                       nothing          Neither  \n",
       "3                       nothing          Neither  \n",
       "4                       nothing          Neither  \n",
       "5                    Hate to AI           Doomer  \n",
       "6         fear of data and hate           Boomer  \n",
       "7                  fear of data           Boomer  \n",
       "8                  fear of data           Boomer  \n",
       "9                   frustration          Neither  \n",
       "10                      nothing           Boomer  \n",
       "11                 fear of data           Boomer  \n",
       "12      Fear of data collection           Boomer  \n",
       "13                        Hater           Boomer  \n",
       "14      Fear of data collection           Boomer  \n",
       "15                 Hater on OSS           Boomer  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO 5: replace this dataset with your own ref using the dataset link above and looking at the \"use\" tab\n",
    "weave_ref = \"weave:///capecape/evals-workshop/object/doomer_or_boomer_dataset:AjXt0JMXffKov9O8MUU2oyr2oVTLYTjIQ9XEva0ZEus\"\n",
    "doomer_or_boomer_dataset = weave.ref(weave_ref).get()\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(doomer_or_boomer_dataset.rows)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 : Evaluations \n",
    "### Components of an Evaluation\n",
    "\n",
    "Evaluations generally consist of four key elements:\n",
    "- An **input prompt** that serves as the basis for the model's completion. This prompt often includes a set of variable inputs that are inserted into a prompt template during testing.\n",
    "- The **output** generated by the model in response to the input prompt.\n",
    "- A **\"gold standard\" answer** used as a reference for assessing the model's output. This can be an exact match that the output must replicate, or an exemplary answer that provides a benchmark for scoring.\n",
    "- A **score**, determined by one of the scoring approaches outlined below, which indicates the model's performance on the question.\n",
    "\n",
    "#TODO 6: Look at the dataset and try to match the input, output, gold standard each row\n",
    "\n",
    "## Evaluation Grading Approaches\n",
    "Evaluations can be time-consuming and costly in two main areas: creating questions and gold standard answers, and the scoring/grading process itself.  \n",
    "Developing questions and ideal answers is often a one-time fixed cost, albeit potentially time-intensive if a suitable dataset is not readily available (consider leveraging an LLM to generate questions!). However, scoring is a recurring expense incurred each time the evaluation is conducted, which is likely to be frequent. Therefore, designing evaluations that can be scored efficiently and economically should be a central priority.\n",
    "\n",
    "![](https://gist.github.com/assets/463317/e970bb03-9552-4712-ba12-727b89928e3b)\n",
    "\n",
    "There are three primary methods for grading (scoring) evaluations:  \n",
    "- **Programmatic:** This approach involves using standard code (primarily string matching and regular expressions) to assess the model's outputs. Common techniques include checking for an exact match against an answer or verifying the presence of key phrase(s) in a string. Programmatic scoring is the most optimal method when feasible, as it is extremely fast and highly reliable. However, not all evaluations are amenable to this style of scoring. \n",
    "  - Goes great with structured output - validate against an enum\n",
    "  - Code generation output - does it run, is valid, does it compile? \n",
    "  - Tool use validation - do the tools exist? \n",
    "- **Human in the loop:** In this approach, a human reviewer examines the model-generated answer, compares it to the gold standard, and assigns a score. While manual scoring is the most versatile method, applicable to nearly any task, it is also exceptionally slow and costly, especially for large-scale evaluations. Designing evaluations that necessitate manual scoring should be avoided whenever possible.\n",
    "  - Domain specific & expert information\n",
    "  - Sensitive topics\n",
    "- **Model-based scoring AKA LLM as a judge:** LLMs (especially Claude, GPT-4o, Gemini) are really good at grading themselves (or even outputs of other LLMs) especially in wide range of tasks that traditionally needed human judgement like tone in creative writing or accuracy in open-ended question, or classification. This model-based scoring is accomplished by creating a _scorer prompt_ for an LLM\n",
    "  - Open ended style questions\n",
    "  - Classification & Translation \n",
    "  - Instruction following\n",
    "\n",
    "Let's explore an example of each\n",
    "\n",
    "## 3.1 Programmatic scoring \n",
    "\n",
    "Here we have a simple programmatic eval that will try and check if the LLM had the right answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m1\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m2\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m3\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m4\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m5\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m6\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m7\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m8\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m9\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m10\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m11\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m12\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m13\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m14\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m15\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m16\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluation summary\n",
       "<span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'string_match'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'match'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'true_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'true_fraction'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5</span><span style=\"font-weight: bold\">}}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'model_latency'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.008246973156929016</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluation summary\n",
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'string_match'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'match'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'true_count'\u001b[0m: \u001b[1;36m8\u001b[0m, \u001b[32m'true_fraction'\u001b[0m: \u001b[1;36m0.5\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'model_latency'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m0.008246973156929016\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üç© https://wandb.ai/capecape/evals-workshop/r/call/019490f1-9ff2-7c50-9cd1-dcb52fcd530c\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'string_match': {'match': {'true_count': 8, 'true_fraction': 0.5}},\n",
       " 'model_latency': {'mean': 0.008246973156929016}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create a programmatic scorer that will compare the ground truth to the LLM answer and check if it is correct\n",
    "os.environ['WEAVE_PRINT_CALL_LINK'] = 'true'\n",
    "import weave\n",
    "from weave import Evaluation\n",
    "\n",
    "# def string_match(output: str, human_annotation: str):\n",
    "#     # check if the model output is exactly the same as human_annotation (Doomer, Boomer, Neither)\n",
    "#     # we expect this evaluation to fail becuase the LLM is talking alot and never returns just the reason\n",
    "#     if not output or not human_annotation:\n",
    "#         raise ValueError(\"Model output or human annotation is empty\")\n",
    "#     return {\"match\": output == human_annotation}\n",
    "\n",
    "\n",
    "def string_match(output: str, human_annotation: str):\n",
    "    # check if model_output includes the human_annotation only once \n",
    "    if human_annotation.lower() in output.lower():\n",
    "        #possible match, now lets check if the model_output includes any of the other options but not the human_annotation\n",
    "        for option in [\"doomer\", \"boomer\", \"neither\"]:\n",
    "            if option.lower() in output.lower() and option.lower() != human_annotation.lower():\n",
    "                return {\"match\": False}\n",
    "        return {\"match\": True}\n",
    "    return {\"match\": False}\n",
    "\n",
    "evaluation = Evaluation(\n",
    "    dataset=doomer_or_boomer_dataset, scorers=[string_match]\n",
    ")\n",
    "\n",
    "@weave.op\n",
    "def function_to_evaluate(input: str):\n",
    "    # here's where you would add your LLM call and return the output\n",
    "    # since we already called the LLM, we can just iterate over the dataset \n",
    "    # and return the llm_classification where the question is the same\n",
    "    row = [row for row in doomer_or_boomer_dataset.rows if row['input'] == input]\n",
    "    return row[0].get('llm_classification')\n",
    "\n",
    "await evaluation.evaluate(function_to_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Structured outputs with programmatic scorers\n",
    "\n",
    "The above example likely gave us a score of 0, because LLMs like to talk, and comparing that via a simple string match is not going to work. \n",
    "\n",
    "Programmatic scorers work great when we have structured outputs and we know exactly what to expect from LLMs. Let's recreate our LLM calls for the same questions with strucutred outputs so we can compare the LLM output directly to the human annotation and see if we can get a better score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['WEAVE_PARALLELISM'] = '5'  # uncomment this to hit the endpoint slower (default is 20)\n",
    "os.environ['WEAVE_PRINT_CALL_LINK'] = 'true'\n",
    "\n",
    "from typing import Literal\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class DoomerOrBoomer(BaseModel):\n",
    "    classification: Literal[\"DOOMER\", \"BOOMER\", \"NEITHER\"] = Field(description=\"The classification of the post, either DOOMER, BOOMER, or NEITHER\", \n",
    "                                                                   example=\"DOOMER\")\n",
    "    reason: str = Field(description=\"The reason for the classification, a short explanation of why the post is classified like this.\")\n",
    "\n",
    "@weave.op\n",
    "def with_structured_llm_call(input: str, displayName: str) -> DoomerOrBoomer:\n",
    "    prompt = f\"\"\"Analyze the following Bluesky post and determine if the author is a:\n",
    "    - DOOMER (someone who hates AI and uses derogatory language)\n",
    "    - BOOMER (someone who doesn't understand AI and asks to remove their data)\n",
    "    - NEITHER (neutral or positive response)\n",
    "    Text to Classify: \n",
    "    \\n\\n {displayName}: \"{input}\"\n",
    "    \"\"\"\n",
    "    \n",
    "    response = completion(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.5,\n",
    "        response_format=DoomerOrBoomer\n",
    "    )\n",
    "    json_out = response.choices[0].message.content\n",
    "    return DoomerOrBoomer.model_validate_json(json_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üç© https://wandb.ai/capecape/evals-workshop/r/call/019490f1-faa8-7f83-9200-efea80cfedc8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classification': 'DOOMER',\n",
       " 'reason': 'The author explicitly states they are a doomer and expresses hatred towards AI, which aligns with the definition of a doomer.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = with_structured_llm_call(\"I'm a doomer because I hate AI\", \"tcapelle\")\n",
    "out.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m1\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m2\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m3\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m4\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m5\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m6\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m7\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m8\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m9\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m10\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m11\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m12\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m13\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m14\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m15\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m16\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluation summary\n",
       "<span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'string_match'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'match'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'true_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'true_fraction'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.75</span><span style=\"font-weight: bold\">}}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'model_latency'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13.258970081806183</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluation summary\n",
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'string_match'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'match'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'true_count'\u001b[0m: \u001b[1;36m12\u001b[0m, \u001b[32m'true_fraction'\u001b[0m: \u001b[1;36m0.75\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'model_latency'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m13.258970081806183\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üç© https://wandb.ai/capecape/evals-workshop/r/call/019490f2-8a3e-72d3-998f-49900727d68a\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'string_match': {'match': {'true_count': 12, 'true_fraction': 0.75}},\n",
       " 'model_latency': {'mean': 13.258970081806183}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def string_match(output: str, human_annotation: str):\n",
    "    # check if the model output is exactly the same as human_annotation (Doomer, Boomer, Neither)\n",
    "    if not output:\n",
    "        raise ValueError(\"Model output is empty\")\n",
    "    \n",
    "    return {\"match\": output.classification.lower() == human_annotation.lower()}\n",
    "\n",
    "new_evaluation = Evaluation(\n",
    "    dataset=doomer_or_boomer_dataset, scorers=[string_match]\n",
    ")\n",
    "\n",
    "await new_evaluation.evaluate(with_structured_llm_call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title { display-mode: \"form\" }\n",
    "\n",
    "calls = weave_api.get_calls(\n",
    "    filter={\"op_names\": [\"weave:///capecape/evals-workshop/op/Evaluation.predict_and_score:UQ7QVKZpY8NfmEYwrnmhiKDBKV0lppySqBZI4XwxfXg\"],\"parent_ids\": [\"019490f2-8a3e-72d3-998f-49900727d68a\"]},\n",
    "    sort_by=[{\"field\":\"started_at\",\"direction\":\"desc\"}],)\n",
    "\n",
    "structured_output_doomer_or_boomer_dataset = []\n",
    "\n",
    "for c in calls:\n",
    "    new_row = dict(c.inputs[\"example\"]).copy()\n",
    "    new_row[\"structured_llm_classification\"] = c.output[\"output\"].classification\n",
    "    new_row[\"structured_llm_reason\"] = c.output[\"output\"].reason\n",
    "    structured_output_doomer_or_boomer_dataset.append(new_row)\n",
    "\n",
    "weave.publish(weave.Dataset(name=\"doomer_or_boomer_dataset_with_structured_output\", \n",
    "                            rows=structured_output_doomer_or_boomer_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 HITL - Human in the loop evaluation grading\n",
    "\n",
    "Programmatic scoring is great for many reasons, cheap to get started with, can run very fast and can be very reliable, but cannot cover open ended questions or tasks that require analysis or judgement. \n",
    "\n",
    "For example, did the LLM follow the instructions it was given, did it hallucinate, was it verbose or concise, etc.\n",
    "\n",
    "To judge those outputs we can use human graders, to provide \"golden answers\", which is what we did above with the annotation example with our Doomer or Boomer app. \n",
    "\n",
    "The downside of HITL is that it's slow, expensive, and not scalable (unless you have a lot of money in the bank). \n",
    "\n",
    "HITL is a great way to kickstart an evaluation dataset and extarpolate with an LLM. \n",
    "\n",
    "Here's a slight alternative on our app, that shows LLM responses and allows our humans in the loop to judge the responses as correct or incorrect. \n",
    "\n",
    "#TODO 11 - Run this app, mark up to 10 responses, and then hit \"run evaluations\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title { display-mode: \"form\" }\n",
    "\n",
    "import weave\n",
    "from weave import Evaluation\n",
    "# weave_ref = \"weave:///capecape/evals-workshop/object/doomer_or_boomer_dataset:AjXt0JMXffKov9O8MUU2oyr2oVTLYTjIQ9XEva0ZEus\"\n",
    "weave_ref = \"weave:///capecape/evals-workshop/object/doomer_or_boomer_dataset_with_structured_output:X4pRm3UDhhd1sNOoTh4l6sHSGWzGXWIwgaOvNy3nW0k\"\n",
    "doomer_or_boomer_dataset_with_structured_output = weave.ref(weave_ref).get()\n",
    "\n",
    "def match_dataset_with_replies():\n",
    "    matched_replies = []\n",
    "    for row in doomer_or_boomer_dataset_with_structured_output.rows:\n",
    "        # Find matching reply in all_replies\n",
    "        for reply in load_replies():\n",
    "            if reply['post']['record']['text'] == row['input']:\n",
    "                matched_reply = {\n",
    "                    'full_reply': reply,\n",
    "                    'input': row.get('input', ''),\n",
    "                    'output': row.get('output', ''),\n",
    "                    'reason': row.get('reason', ''),\n",
    "                    'llm_classification': row.get('llm_classification', ''),\n",
    "                    'displayName': row.get('displayName', '')\n",
    "                }\n",
    "                matched_replies.append(matched_reply)\n",
    "                break\n",
    "    return matched_replies\n",
    "\n",
    "matched_replies = match_dataset_with_replies()\n",
    "annotated_rows = []\n",
    "\n",
    "def get_next_annotated_post(current_index:int = 0):\n",
    "    # Get the matched replies\n",
    "    \n",
    "    print(current_index, len(matched_replies))\n",
    "    if current_index >= len(matched_replies):\n",
    "        current_index = 0  # Reset to beginning if we've reached the end\n",
    "        \n",
    "    reply = matched_replies[current_index]\n",
    "    post = reply['full_reply']\n",
    "    \n",
    "    # Format the post data for the template\n",
    "    created_at = datetime.fromisoformat(post['post']['record']['createdAt'].replace('Z', '+00:00'))\n",
    "    formatted_date = created_at.strftime('%b %d, %Y, %I:%M %p')\n",
    "    \n",
    "    # Convert AT URI to bsky.app URL\n",
    "    at_uri = post['post']['uri']\n",
    "    _, _, author_did, _, post_id = at_uri.split('/')\n",
    "    post_url = f\"https://bsky.app/profile/{post['post']['author']['handle']}/post/{post_id}\"\n",
    "    \n",
    "    post_data = {\n",
    "        'author': post['post']['author'],\n",
    "        'created_at': formatted_date,\n",
    "        'text': post['post']['record']['text'],\n",
    "        'like_count': post['post'].get('likeCount', 0),\n",
    "        'repost_count': post['post'].get('repostCount', 0),\n",
    "        'has_image': False,\n",
    "        'post_url': post_url\n",
    "    }\n",
    "    \n",
    "    # Use the stored LLM classification and human annotation\n",
    "    analysis = f\"\"\"LLM Classification: {reply['llm_classification']}\n",
    "    \n",
    "LLM Reasoning: {reply['reason']}\n",
    "    \"\"\"\n",
    "    \n",
    "    run_evaluation_btn = {\n",
    "        \"interactive\": True if len(annotated_rows) >=  10 else False,\n",
    "        \"value\": \"Run Evaluation\" if len(annotated_rows) >=  10 else f\"Annotate {10 - len(annotated_rows)} more posts\"\n",
    "    }\n",
    "    return template.render(**post_data), analysis, current_index + 1, gr.update(**run_evaluation_btn), \"\"\n",
    "\n",
    "def submit_hitl_feedback(correct_or_incorrect: str, feedback: str, next_index: int):\n",
    "    annotated_rows.append({\n",
    "        \"input\": matched_replies[next_index-1].get('input'),\n",
    "        \"output\": matched_replies[next_index-1].get('output'),\n",
    "        \"llm_classification\": matched_replies[next_index-1].get('llm_classification'),\n",
    "        \"correct_or_incorrect\": True if correct_or_incorrect == \"correct\" else False,\n",
    "        \"human_reason_for_correct_or_incorrect\": feedback,\n",
    "    })\n",
    "    return get_next_annotated_post(next_index)\n",
    "\n",
    "\n",
    "def right_according_to_human(output: str, correct_or_incorrect: bool):\n",
    "    return correct_or_incorrect\n",
    "\n",
    "@weave.op\n",
    "def return_input_row(input: str):\n",
    "    return [x for x in annotated_rows if x.get('input') == input]\n",
    "\n",
    "async def run_evaluation():\n",
    "    hitl_evaluation = Evaluation(\n",
    "        dataset=annotated_rows,\n",
    "        scorers=[right_according_to_human],\n",
    "        name=\"hitl_evaluation\"\n",
    "    )\n",
    "    \n",
    "    result = await hitl_evaluation.evaluate(return_input_row)\n",
    "    gr.Info('Evaluation complete! Check your Weave project for the results.')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 16\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @title { display-mode: \"form\" }\n",
    "# @markdown Run this cell to start the HITL evaluation app\n",
    "\n",
    "# %%blocks\n",
    "# Create a Gradio Blocks app\n",
    "os.environ['WEAVE_PRINT_CALL_LINK'] = 'true'\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as new_demo:\n",
    "    # Add a title and description\n",
    "    gr.Markdown(\"\"\"\n",
    "    # Human in the loop\n",
    "    \"\"\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(f\"\"\"## 1. Post to Analyze  \"\"\")\n",
    "            post_html = gr.HTML()\n",
    "            # next_post_btn = gr.Button(\"Skip Post & Analyze Another\", variant=\"primary\")\n",
    "            gr.Markdown(f\"\"\"\n",
    "            #### Instructions for HTIL judge: \n",
    "            - review LLM outputs and mark them as correct or incorrect  \n",
    "            - after 10-20 examples, hit \"run evaluation\" button\n",
    "    \n",
    "            See your Weave project & traces [here](https://wandb.ai/{weave_api._project_id()})\n",
    "            \"\"\")\n",
    "            \n",
    "        \n",
    "        with gr.Column(scale=2):\n",
    "            \n",
    "            analysis_output = gr.Textbox(\n",
    "                label=\"2. Review LLM Classification for this post\",\n",
    "                placeholder=\"Analysis will appear here...\",\n",
    "                lines=4,\n",
    "            )\n",
    "            next_index = gr.State(value=0)\n",
    "            \n",
    "            with gr.Accordion(\"Reminder of Doomer, Boomer, or Neither Criteria\", open=False):\n",
    "                gr.Markdown(f\"\"\"\n",
    "                `Doomer`: Someone who hates AI, and uses derogatory language towards the author of the post because of thier hate for AI and their data being used for AI  \n",
    "                `Boomer`: Someone who doesn't understand AI, and copy-pastes a request to remove their data from the dataset  \n",
    "                `Neither`: Folks who reply neutral or positive to the post.\n",
    "                \"\"\")\n",
    "            # Replace dropdown with three buttons\n",
    "            reason_input = gr.Textbox(label=\"3. Add reason and submit\",placeholder=\"Reason why the LLM got this classification right or wrong\", lines=2)\n",
    "            with gr.Row():\n",
    "                correct_btn = gr.Button(\"LLM is Correct üëç\")\n",
    "                incorrect_btn = gr.Button(\"LLM is Incorrect üëé\")\n",
    "\n",
    "            run_evaluation_btn = gr.Button(\"Run Evaluation\", variant=\"primary\", interactive=False)\n",
    "\n",
    "            \n",
    "    # Set up event handler for combined next/analyze\n",
    "    # next_post_btn.click(fn=get_next_annotated_post, inputs=[next_index], outputs=[post_html, analysis_output, next_index, run_evaluation_btn, reason_input])\n",
    "    \n",
    "    correct_btn.click(fn=submit_hitl_feedback, inputs=[gr.State(\"correct\"), reason_input, next_index], outputs=[post_html, analysis_output, next_index, run_evaluation_btn, reason_input])\n",
    "    incorrect_btn.click(fn=submit_hitl_feedback, inputs=[gr.State(\"incorrect\"), reason_input, next_index], outputs=[post_html, analysis_output, next_index, run_evaluation_btn, reason_input])\n",
    "\n",
    "    run_evaluation_btn.click(fn=run_evaluation, inputs=[], outputs=[analysis_output])\n",
    "    # Initialize with first post and analysis\n",
    "    post_html.value, analysis_output.value, next_index.value, run_evaluation_btn.value, reason_input.value = get_next_annotated_post()\n",
    "\n",
    "new_demo.queue()\n",
    "new_demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 LLM as a Judge - use another LLM to grade your LLM outputs\n",
    "\n",
    "Having to manually grade the above eval every time is going to get very annoying very fast, especially if the eval is a more realistic size (dozens, hundreds, or even thousands of questions). Luckily, there's a better way! \n",
    "\n",
    "We can actually have an LLM do the grading for us. We'll use a teacher model to grade the LLM outputs of a \"student\" model (in this case the LLM we're using for our production system is the student). \n",
    "\n",
    "There are a few issues with this approaches to be aware of: \n",
    " - LLMs are not great at numerical scoring (eg 1-5) \n",
    " - The order of canditate responses matter\n",
    " - Foundational models tend to prefer their own outputs over other models\n",
    " - LLMs prefer longer respones and \"style\" over accuracy\n",
    "\n",
    "\n",
    "## 3.3.1 Let's build our LLM judge\n",
    "\n",
    "First, we'll start by building a \"grader prompt\" template, a prompt asking our judge to perform the judging itself. This will be our iteration grounds. In this template, we'll inject both the output of our production LLM model, and the criteria / rules or rubric that makes an answer correct or incorrect. \n",
    "\n",
    "In our case, the classification into one of 3 (Doomer, Boomer, Neither) is done \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m1\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m2\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m3\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m4\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m5\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m6\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m7\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m8\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m9\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m10\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m11\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m12\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m13\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m14\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m15\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m16\u001b[0m of \u001b[1;36m16\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluation summary\n",
       "<span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'right_according_to_llm_judge'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'match'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'true_count'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'true_fraction'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8125</span><span style=\"font-weight: bold\">}}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'model_latency'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19.960527390241623</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluation summary\n",
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'right_according_to_llm_judge'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'match'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'true_count'\u001b[0m: \u001b[1;36m13\u001b[0m, \u001b[32m'true_fraction'\u001b[0m: \u001b[1;36m0.8125\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'model_latency'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m19.960527390241623\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üç© https://wandb.ai/capecape/evals-workshop/r/call/01949102-fe55-7813-9667-5c1ecb687040\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'right_according_to_llm_judge': {'match': {'true_count': 13,\n",
       "   'true_fraction': 0.8125}},\n",
       " 'model_latency': {'mean': 19.960527390241623}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1 - Build a grader prompt\n",
    "import weave\n",
    "from weave import Evaluation\n",
    "from typing import Literal\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class DoomerOrBoomer(BaseModel):\n",
    "    classification: Literal[\"DOOMER\", \"BOOMER\", \"NEITHER\"] = Field(description=\"The classification of the post, either DOOMER, BOOMER, or NEITHER\", example=\"DOOMER\")\n",
    "    reason: str = Field(description=\"The reason for the classification, a short explanation of why the post is classified like this.\")\n",
    "\n",
    "\n",
    "def build_grader_prompt(displayName: str, input: str, llm_classification: str): \n",
    "    system_prompt = f\"\"\"You are a judge who is grading the output of an automated assistant.\n",
    "    \n",
    "    ## Inputs\n",
    "    You are provided with the following: \n",
    "    - A comment made on social media and the handle of the person making the comment\n",
    "    - The classification output of an automated assistant made about the comment\n",
    "    - A set of guidelines and additional context for you to understand the input and the correct way to classify it\n",
    "    \n",
    "    ## Instructions of how to classify responders: \n",
    "    `Doomer`: Someone who hates AI, and uses derogatory language towards the author of the post because of their hate for AI and their data being used for AI.\n",
    "    `Boomer`: Someone who doesn't understand AI, and copy-pastes a request to remove their data from the dataset. He is a boomer because he is old and doesn't understand AI.\n",
    "    `Neither`: Folks who reply neutral or positive to the post.\n",
    "\n",
    "    Your task is to classify the comment into one of the 3 categories and give a short reasoning for your choice.\n",
    "    \"\"\"\n",
    "\n",
    "    grader_prompt_template = f\"\"\"\n",
    "    <input>\n",
    "    @{displayName}: {input}\n",
    "    </input>\n",
    "\n",
    "    <automated_assistant_classification>\n",
    "    {llm_classification}\n",
    "    </automated_assistant_classification>\n",
    "    \"\"\"\n",
    "\n",
    "    return system_prompt, grader_prompt_template\n",
    "\n",
    "# Step 2 - Get our datasets \n",
    "weave_ref = \"weave:///capecape/evals-workshop/object/doomer_or_boomer_dataset_with_structured_output:X4pRm3UDhhd1sNOoTh4l6sHSGWzGXWIwgaOvNy3nW0k\"\n",
    "doomer_or_boomer_dataset_with_structured_output = weave.ref(weave_ref).get()\n",
    "\n",
    "\n",
    "# Step 3 - Build our LLM Judge API function \n",
    "@weave.op\n",
    "def llm_judge_api(input: str, structured_llm_classification: str, displayName: str):\n",
    "    system_prompt, grader_prompt = build_grader_prompt(displayName, input, structured_llm_classification)\n",
    "    response = completion(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": grader_prompt}],\n",
    "        temperature=0,\n",
    "        response_format=DoomerOrBoomer,\n",
    "    )\n",
    "    json_out = response.choices[0].message.content\n",
    "    return DoomerOrBoomer.model_validate_json(json_out)\n",
    "\n",
    "# Step 4 - Create a scorer \n",
    "def right_according_to_llm_judge(output: dict, structured_llm_classification: str):\n",
    "    return {\"match\": structured_llm_classification.lower() == output.classification.lower()}\n",
    "\n",
    "# Step 5 - Run our evaluation \n",
    "llm_judge_evaluation = Evaluation(\n",
    "    dataset=doomer_or_boomer_dataset_with_structured_output,\n",
    "    scorers=[right_according_to_llm_judge],\n",
    "    name=\"LLM Judge Evaluation\"\n",
    ")\n",
    "\n",
    "await llm_judge_evaluation.evaluate(llm_judge_api, __weave={\"display_name\": \"LLM Judge\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Aligning our judges with human preferences - Meta evaluation\n",
    "\n",
    "This is a bit out of scope for our workshop, but for those who want to learn more, one we start running our LLM as a judge, we'll notice their shortcomings. They will be biased toward certain things, changing the order of the questions sometimes will yield different results etc' \n",
    "\n",
    "Also, the human graders understanding of the question will change during the annotation process itself. \n",
    "\n",
    "So a meta evaluation process is needed to understand how the judge itself is performing, and align the LLM judge with the additional inputs from HITL responses. \n",
    "\n",
    "Then we need to compare between the judges to empirically contrast and understand if we made a material difference. \n",
    "\n",
    "For more of a deep dive into this topic, W&B just published a course on evaluations, https://wandb.me/evals with more info\n",
    "\n",
    "# Recap and Additional resources\n",
    "\n",
    "You've made it all the way to the end of this notebook! By now you have got a hands on experience in implementing nearly all parts of the robust LLMs in production framework below: \n",
    "\n",
    "![three](https://gist.github.com/user-attachments/assets/0d51de65-8ec7-4cc5-a102-5a13229f5531)\n",
    "\n",
    "## Additional resources\n",
    "\n",
    "- Weave documentation - [weave docs](https://wandb.me/weave)\n",
    "- W&B Evaluations course - [evals course](https://wandb.me/evals)\n",
    "- Eugene Yan's excellent blog - [evaluating LLM evaluatiors](https://eugeneyan.com/writing/llm-evaluators/)\n",
    "- Who validates the validators - Shreya Shankar [Paper](https://arxiv.org/abs/2404.12272)\n",
    "- Hamel Housain - [your product needs evaluations](https://hamel.dev/blog/posts/evals/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weave",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
